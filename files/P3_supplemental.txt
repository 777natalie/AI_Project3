
######################
# Supplemental Questions #
######################


Answer the supplemental questions here! Make sure you follow the format if it is asked
Q1#######################
QS1.1: Explain how you implemented computeActionFromValues(state) and add your explanation to P3_supplmental.txt

The function computeActonFromValues(state) is supposed to compute the best action according to the value 
function given by self.values. So first we check if there are even any actions left, just so we can handle that case. 
We do that with if self.mdp.isTerminal(state):, and then after we confirm there are actions we then go and get the next
possible actions with actions = self.mdp.getPossibleActions(state)

Now after we grabbed all of the actions we want to compute the best values from each action. So we compute the q values of 
each and store it into qValue, we iterate through each state to compute this with the for loop, for action in actions, we go 
through each one and call our computeQValuesFromValues function to get the q values, then retun back the best (most optimal
based off of values) q value with return qValue.argMax()

QS1.2: Explain how you implemented computeQvaluesfromValues(state,action) and add your explanation to P3_supplement.txt.

In the project description, computeQValuesFromValues is supposed to return the Q value of the state and action pair given by self.values
so we did this by first grabbing that state in the next transition so that we can use the state and action. We grab the next
state with nextActions = self.mdp.getTransitionStatesAndProbs(state,action). Once we gather the actions we will go into each to use
their states and actions with the for loop for actions in nextActions:. 

In this, we compute the q value of each state and its action, this takes into account the discount you gave us and is made
after the formula given to properly compute the q value
   value += actions[1] * (self.mdp.getReward(state, action,actions[0])+self.discount*self.values[actions[0]])
after we compute it using the formula, we return that. This is used for our other function computeActionFromValues to choose the best
out of all of these q values. 


Q3#######################

QS3.1: 

A. Prefer the close exit (+1), risking the cliff (-10):

discount = 0.5
noise = 0
living reward = -1.0

Since the agent prefers the closer exit, a lower discount and no noise is necessary to get the
shortest path and closest reward. By making the living reward negative, this forces the agent to be 
quicker despite the risk of the cliff.

B. Prefer the close exit (+1), but avoiding the cliff (-10):

discount = 0.3
noise = 0.2
living reward = -0.1

To achieve the closer exit, a lower discount is makes the agent want the nearest reward. As for the noise,
0 wasn't necessary since the agent is just avoiding the cliff and not risking it. The living reward isn't as
strong as a penalty since the agent needs to be a little more cautious with its path. 

C. Prefer the distant exit (+10), risking the cliff (-10):

discount: 0.9
noise: 0
living reward: -0.1

With a higher discount means the agent will search for a higher reward. This means it will prefer the distant
(+10) exit. Noise isn't necessary since it is able to take risks with the cliff. As for the living reward, the duration of the agents path doesn't need to be quick but it also doesn't care so much about the cliff. Due to this, I chose a small penalty. 

D. Prefer the distant exit (+10), avoiding the cliff (-10):

discount: 0.9
noise: 0.3
living reward: -0.1

Like part C a high discount and small penalty is needed for the agent to choose the distant exit. The difference
is that with a small noise parameter, the agent will choose to be more cautious of the cliff. 

E. Avoid both exits and the cliff (so an episode should never terminate):

discount: 0.0
noise: 0.0
living reward: 10.0

In order to avoid both exits and the cliffs, the agent should't care about the reward or risk if the living
reward is extremely high. This will cause it to prioritize living and never terminating. 


Q5#######################

QS5.1:

In order to implement my Q-learning agent, I initialized a dictionary Qvals that keeps track of states and 
their actions and the corresponding Q values. This allowed me to know what states were visited, terminal states, and update states. I then was able to retrieve the Q values in the getQValue function by searching through the dictionary Qvals. 

To get the maximum Q value in computeValueFromQValues, I checked whether a state had legal actions or not and then searched through the list of legal actions for the given state in order to retrieve the max Q value.

Next I was able to choose the best (legal) action for a state. This was done by finding all actions
with the maximum Q value. Using the previous function, I was able to find the max Q value, make a list of best actions (since there could be ties) and compared each Q value that corresponded to the state and its action to the max Q value. I returned a random choice from the best actions list to break the tie. 

As for updating Q values I used the Bellman equation. To help determine future rewards in the Bellman implementation, I retrieved the maximum Q value from nextState and then added the new Q value to the original QVals dictionary to update the new state with its Q value.

QS5.2 [optional]: 

Q6#######################

QS6.1:
The behavior of the agent changed how I expected. With a lower epsilon value the agent makes less random decisions and doesn't utilize exploration as much. It stuck to the same path and didn't necessarily choose the best path. As for the 0.9 value the agent began to explore more and more and took a lot more time. This is because the exploration rate was much higher than previously. 

QS6.2 [optional]:


Q7#######################
QS7.1

There was not an epsilon or a learning rate that caused the optimal policy to be learned after 50 iterations.
Thus, NOT POSSIBLE is returned. This is likely due to the agents exploration becoming more random over time, causing it to miss states. This means the greater than 99% confidence isn't achievable.





